---
{"title":"Text Generation with CLM","date_created":"2023-04-25","date_modified":"2023-04-25","dg-publish":true,"alias":"Text Generation with CLM","dg-path":"nlp/huggingface-clm-generation.md","permalink":"/nlp/huggingface-clm-generation/","dgPassFrontmatter":true,"created":"2023-04-25","updated":"2023-04-25"}
---


ë³¸ ë¬¸ì„œì—ì„œëŠ” CLM(Causal Language Model)ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.

## CLM(Causal Language Model)

- Causal Language Model(CLM) : Causal Language Modeling taskì— ëŒ€í•´ í•™ìŠµëœ language model. ex) GPT
    - Causal Language Modeling task : ì• tokenë“¤ë¡œë¶€í„°, ê·¸ ë‹¤ìŒ tokenì„ ë§ì¶”ëŠ” task
        - í…ìŠ¤íŠ¸ë¥¼ ë‹¨ë°©í–¥(uni-direction)ìœ¼ë¡œë§Œ ë³¼ ìˆ˜ ìˆë‹¤.
        - auto-regressive
    - ì¼ë°˜ì ìœ¼ë¡œ ìì—°ì–´ ìƒì„±(NLG, natural language generation) taskì— ê°•í•˜ë‹¤ê³  ì•Œë ¤ì ¸ ìˆìŒ
    - ì˜¤ëŠ˜ë‚ ì—ëŠ” transformerì˜ decoder êµ¬ì¡°ë¥¼ ì´ìš©í•´ì„œ ì£¼ë¡œ ë§Œë“¦ â†’ decoder ëª¨ë¸ì´ë¼ ë¶€ë¥´ê¸°ë„ í•¨

- cf) Masked Language Model(MLM) : Masked Language Modeling taskì— ëŒ€í•´ í•™ìŠµëœ language model. ex) BERT
    - Masked Language Modeling task : ì•ë’¤ tokenë“¤ë¡œë¶€í„°, ê°€ë ¤ì§„(masked) tokenì„ ë§ì¶”ëŠ” task.
        - í…ìŠ¤íŠ¸ë¥¼ ì–‘ë°©í–¥(bidirectional)ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.
        - auto-encoding
    - ì¼ë°˜ì ìœ¼ë¡œ ìì—°ì–´ ì´í•´(NLU, natural language understanding) taskì— ê°•í•˜ë‹¤ê³  ì•Œë ¤ì ¸ ìˆìŒ
    - ì˜¤ëŠ˜ë‚ ì—ëŠ” transformerì˜ encoder êµ¬ì¡°ë¥¼ ì´ìš©í•´ì„œ ì£¼ë¡œ ë§Œë“¦ â†’ encoder ëª¨ë¸ì´ë¼ ë¶€ë¥´ê¸°ë„ í•¨

## Text Generation

### generation task == search task

- objective : 'ê°€ì¥ ê·¸ëŸ´ë“¯í•œ' sequenceë¥¼ ì°¾ëŠ” ê²ƒ = ê°€ì¥ í™•ë¥ ì´ ë†’ì€ sequenceë¥¼ ì°¾ëŠ” ê²ƒ
- chain ruleì— ì˜í•´, sequenceì˜ í™•ë¥ ì€ sequenceë¥¼ êµ¬ì„±í•˜ê³  ìˆëŠ” ê° tokenë“¤ì˜ (ì´ì „ tokenì´ ì£¼ì–´ì¡Œì„ ë•Œ) ë“±ì¥ í™•ë¥ ì˜ ê³±ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤.
    - ì¼ë°˜ì ìœ¼ë¡  ê³„ì‚° ì•ˆì •ì„±ì„ ìœ„í•´ì„œ í™•ë¥ ì˜ ê³± ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê¸°ë³´ë‹¨ log í™•ë¥ ì˜ í•© ì—°ì‚°ì„ ìˆ˜í–‰
    - ìì„¸í•œ ì„¤ëª…ì€ [[Knowledge Base/nlp/language-model\|Language Model]] ë¬¸ì„œ ì°¸ì¡°
- ì¦‰ ìƒì„± ë¬¸ì œ = ìµœì  ê²½ë¡œ íƒìƒ‰ ë¬¸ì œ

### CLMì—ì„œì˜ text generation

- ê¸°ë³¸ idea : `t`ê°œì˜ tokenìœ¼ë¡œ êµ¬ì„±ëœ sequenceê°€ CLMì— ì…ë ¥ë˜ì—ˆì„ ë•Œ, ë§ˆì§€ë§‰ tokenì˜ ë§ˆì§€ë§‰ layerì—ì„œì˜ hidden stateì—ëŠ” ë‹¤ìŒ `t + 1`ë²ˆì§¸ tokenì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ì •ë³´ê°€ ë“¤ì–´ ìˆë‹¤.

- ë§ˆì§€ë§‰ tokenì˜ ë§ˆì§€ë§‰ decoder layerì—ì„œì˜ hidden stateë¥¼ LM Headì— ì…ë ¥í•˜ë©´, vocabularyì— ìˆëŠ” ê° tokenë“¤ì´ ë‹¤ìŒ tokenìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì ì ˆí•œì§€ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” logitì„ ì–»ì„ ìˆ˜ ìˆë‹¤.
    - LM Head : fully-connected(= linear) layer (`Linear(hidden_size, vocab_size)`)

- LM headì˜ ì¶œë ¥ê°’(ì¦‰, logit)ì— softmaxë¥¼ ì”Œìš°ë©´, vocabularyì— ìˆëŠ” ê° tokenë“¤ì´ ë‹¤ìŒ tokenìœ¼ë¡œ ì˜¬ ìˆ˜ ìˆëŠ” í™•ë¥ (probability)ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.

### ê¸°íƒ€

- (ê¸¸ì´ê°€ ë‹¤ë¥¸) batch ì…ë ¥ì„ ì²˜ë¦¬í•˜ë ¤ë©´, paddingì„ ì™¼ìª½ì— ì£¼ì–´ì•¼ í•œë‹¤.
    - ì¼ë°˜ì ìœ¼ë¡œ í•˜ë“¯ì´ ì˜¤ë¥¸ìª½ì— paddingì„ ì£¼ê²Œ ë˜ë©´, batch ë‚´ ëª‡ëª‡ exampleì€ ë§ˆì§€ë§‰ tokenì— ë„ë‹¬í–ˆì§€ë§Œ(ê·¸ë˜ì„œ ì´ì œ generationì„ ì‹œì‘í•´ì•¼ í•˜ì§€ë§Œ) ë‚˜ë¨¸ì§€ exampleì€ ì•„ì§ ë„ë‹¬í•˜ì§€ ì•Šì€(generationì„ ì‹œì‘í•  ìˆ˜ ì—†ëŠ”) ìƒíƒœê°€ ë  ìˆ˜ ìˆìŒ.
    - ì™¼ìª½ paddingì„ ì‚¬ìš©í•´ì•¼ batch ë‚´ ëª¨ë“  exampleë“¤ì˜ ë§ˆì§€ë§‰ tokenë“¤ì„ ê°™ì€ ìœ„ì¹˜(ë§ˆì§€ë§‰ ì—´)ì— ëª¨ì„ ìˆ˜ ìˆë‹¤.
        - ì–´ì°¨í”¼ paddingì´ ë“¤ì–´ê°€ëŠ” ê³³ì—ëŠ” attention maskê°€ ì”Œì›Œì§€ê¸°ì— í° ë¬¸ì œê°€ ì—†ë‹¤.
    - ğŸ¤—huggingface transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ tokenizerì—ì„œëŠ” `padding_side="left"`ë¥¼ ì£¼ì–´ ì†ì‰½ê²Œ ì™¼ìª½ paddingì„ ì¤„ ìˆ˜ ìˆë‹¤.
        ```python title:"left padding" linenos highlight:"3"
        from transformers import AutoTokenizer

        tokenizer = AutoTokenizer.from_pretrained("gpt2", padding_side="left")
        tokenizer.pad_token_id = tokenizer.eos_token_id # gpt2ì˜ tokenizerì—ëŠ” pad_tokenì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—, eos_tokenì„ pad_tokenìœ¼ë¡œ ì‚¬ìš©í•˜ë¼ê³  ì•Œë ¤ì¤˜ì•¼ í•œë‹¤.

        texts = [
            "Hello",
            "I love",
            "I want to"
        ]

        tokenized = tokenizer(texts, padding=True, return_tensors="pt")

        print(tokenized.input_ids)
        # tensor([[50256, 50256, 15496],
        #         [50256,    40,  1842],
        #         [   40,   765,   284]])

        print(tokenized.attention_mask)
        # tensor([[0, 0, 1],
        #         [0, 1, 1],
        #         [1, 1, 1]])
        ```

## Text Generation Strategy

- í¬ê²Œ deterministicí•œ ë°©ë²•ê³¼ stochasticí•œ ë°©ë²•ìœ¼ë¡œ ë¶„ë¥˜ ê°€ëŠ¥
    - deterministic : greedy search, beam search
    - stochastic : top-k sampling, top-p sampling(nucleus sampling)
    - ê¸°íƒ€ : contrastive search

### Greedy Search

#### Idea

![greedy search](https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png)

- ë§¤ ì‹œì ë§ˆë‹¤ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ tokenë“¤ë§Œì„ ê³„ì†í•´ì„œ ì„ íƒí•˜ëŠ” ë°©ì‹
    - ì¦‰ modelì— ì´ì „ê¹Œì§€ì˜ ì…ë ¥/ìƒì„± tokenì„ ì…ë ¥í•˜ê³  ê·¸ ê°’ìœ¼ë¡œ ë‹¤ìŒ tokenì„ ì˜ˆì¸¡í•˜ëŠ” ì—°ì‚°ì„ `max_new_token`ë²ˆ ìˆ˜í–‰í•´ì•¼ í•œë‹¤.
    - ì˜ˆë¥¼ ë“¤ì–´ ìœ„ ê·¸ë¦¼ì—ì„  ("The", "nice", "woman")ì´ ì„ íƒëœë‹¤.

```python title:"greedy search : skeleton code" linenos highlight:"27"
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"

# tokenizer, model ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer =  AutoTokenizer.from_pretrained("gpt2", padding_side="left")
tokenizer.pad_token_id = tokenizer.eos_token_id # gpt2ì˜ tokenizerì—ëŠ” pad_tokenì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—, eos_tokenì„ pad_tokenìœ¼ë¡œ ì‚¬ìš©í•˜ë¼ê³  ì•Œë ¤ì¤˜ì•¼ í•œë‹¤.

model = AutoModelForCausalLM.from_pretrained("gpt2", pad_token_id=tokenizer.eos_token_id).to(device) # ë§ˆì°¬ê°€ì§€

# tokenization
texts = ["Hello", "I love", "I want to"]

tokenized = tokenizer(texts, padding=True, return_tensors="pt").to(device)
input_ids, attention_mask = tokenized.input_ids, tokenized.attention_mask # tensor[batch_size, seq_len]

# generation
max_new_tokens = 5
for _ in range(max_new_tokens):
    with torch.no_grad():
        model_inputs = model.prepare_inputs_for_generation(input_ids, attention_mask=attention_mask) # í…ìŠ¤íŠ¸ ìƒì„±ì— í•„ìš”í•œ ì…ë ¥ê°’ì„ ê°€ê³µí•´ ì£¼ëŠ” method
        model_output = model(**model_inputs)
    
    # next token ê³„ì‚° : argmax ì—°ì‚°ì„ ì´ìš©, logit ê°’ì´ ê°€ì¥ í° tokenì„ ì„ íƒ(greedy search)
    next_token_logits = model_output.logits[:, -1, :] # tensor[batch_size, vocab_size]
    next_tokens = next_token_logits.argmax(dim=1).reshape(-1, 1) # tensor[batch_size, 1]
    
    # prepare for next iteration
    input_ids = torch.cat([input_ids, next_tokens], dim=1) 
    attention_mask = torch.cat([attention_mask, torch.ones_like(next_tokens)], dim=1)

print(tokenizer.batch_decode(input_ids, skip_special_tokens=True))
# ["Hello, I'm sorry,", "I love the way you're doing", 'I want to be able to do that']
```

- ì½”ë“œ í•´ì„¤
    - line 20 - 35 : `max_new_tokens`ë²ˆ iterationì„ ëŒë©° next tokenì„ ìƒì„±
    - line 22 : `model.prepare_inputs_for_generation()`
        - modelì´ í…ìŠ¤íŠ¸ ìƒì„±ì„ ì§„í–‰í•  ìˆ˜ ìˆë„ë¡ (model-specificí•˜ê²Œ) ì…ë ¥ê°’ì„ ê°€ê³µí•´ ì£¼ëŠ” method
        - í›„ìˆ í•  `.generate()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ëª¨ë¸ì€ ë°˜ë“œì‹œ ì´ ë©”ì†Œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•œë‹¤.
    - line 26 :  ë§ˆì§€ë§‰ tokenì— ëŒ€í•œ logit ê°’
        - ë§ˆì§€ë§‰ tokenì˜ ë§ˆì§€ë§‰ decoder layerì—ì„œì˜ hidden state ê°’ì„ LM headì— ë„£ì€ ê°’
    - line 27 : argmax ì—°ì‚°ì„ ì´ìš©, logit ê°’ì´ ê°€ì¥ í° tokenì„ ì„ íƒ (greedy search)
    - line 30, 31 : ë‹¤ìŒ iterationì„ ìœ„í•´ ì…ë ¥ê°’ ë’¤ì— ìƒˆë¡œ ë§Œë“¤ì–´ì§„ tokenì„ ë¶™ì„

> [!warning] 
> ì´ êµ¬í˜„ì€ greedy searchì˜ ë¼ˆëŒ€ë§Œ ê°„ë‹¨íˆ êµ¬í˜„í•œ ê²ƒìœ¼ë¡œ, ì‹¤ì œë¡œ ì‚¬ìš©í•˜ê¸°ì—” ë¬´ë¦¬ê°€ ìˆë‹¤.
> - ì˜ˆë¥¼ ë“¤ì–´ `<eos>` token ë“±ì¥ ì‹œ early stopì„ ìˆ˜í–‰í•œë‹¤ë˜ì§€ í•˜ëŠ” ê¸°ëŠ¥ë“¤ì€ ëª¨ë‘ ë¹ ì ¸ ìˆë‹¤.
> - greedy searchë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ í›„ìˆ í•  `.generate()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì.

#### ì¥/ë‹¨ì 

- ì¥ì  : 
    - ë¹ ë¥´ë‹¤.
    - ì–¸ì œë‚˜ ê°™ì€ ê°’ì„ ë‚´ë†“ëŠ”ë‹¤(deterministic).
    - ë©”ëª¨ë¦¬ ì†Œë¹„ëŸ‰ ë§¤ìš° ì ìŒ
- ë‹¨ì  :
    - ìµœì„ ì˜ ì •ë‹µì´ ì•„ë‹ ìˆ˜ ìˆë‹¤.
        - ë§¤ ìˆœê°„ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ tokenì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ sequenceë¥¼ ì„ íƒí•œë‹¤ëŠ” ê²ƒì„ ë³´ì¥í•´ì£¼ì§€ ì•ŠëŠ”ë‹¤.
        - ì§€ê¸ˆì˜ ìµœì„ ì´ ë‚˜ì¤‘ì—ëŠ” ë‚˜ìœ ì„ íƒì¼ ìˆ˜ ìˆë‹¤.
            - ì• tokenë“¤ì˜ í™•ë¥ ì€ ë‚®ì•„ë„, ë’· tokenë“¤ë¡œ ê°€ë©´ í™•ë¥ ì´ ë§¤ìš° ë†’ì•„ ìµœì¢… sequenceì˜ í™•ë¥ ì´ ë” ë†’ì•„ì§ˆ ìˆ˜ë„ ìˆë‹¤.
    - ë³„ë¡œ ì°¸ì‹ í•œ(surprising) í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ëª»í•œë‹¤.

#### ì‚¬ìš©ë²•

- ğŸ¤—huggingface transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `generate()` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©´ ì†ì‰½ê²Œ greedy searchë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.
    - `.generate()` ë©”ì†Œë“œ í˜¸ì¶œ ì‹œ `num_beams=1`, `do_sample=False`ë¡œ ì£¼ë©´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

        ```python title:".generate() ë©”ì†Œë“œ ì´ìš©í•´ greedy search ìˆ˜í–‰í•˜ê¸°" linenos highlight:"27,28"
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # tokenizer, model ë¶ˆëŸ¬ì˜¤ê¸°
        tokenizer =  AutoTokenizer.from_pretrained("gpt2", padding_side="left")
        tokenizer.pad_token_id = tokenizer.eos_token_id # gpt2ì˜ tokenizerì—ëŠ” pad_tokenì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—, eos_tokenì„ pad_tokenìœ¼ë¡œ ì‚¬ìš©í•˜ë¼ê³  ì•Œë ¤ì¤˜ì•¼ í•œë‹¤.
        
        model = AutoModelForCausalLM.from_pretrained("gpt2", pad_token_id=tokenizer.eos_token_id).to(device) # ë§ˆì°¬ê°€ì§€
        
        # tokenization
        texts = [
            "Hello",
            "I love",
            "I want to"
        ]
        
        tokenized = tokenizer(texts, padding=True, return_tensors="pt").to(device)
        
        # generation
        output = model.generate(
            **tokenized,
            max_new_tokens=5, # (ìµœëŒ€) 5ê°œì˜ tokenë§Œ ìƒì„±
            return_dict_in_generate=True, # ì¶œë ¥ê°’ì„ tuple í˜•íƒœê°€ ì•„ë‹Œ dict í˜•íƒœë¡œ ë°›ê¸° ìœ„í•œ ì˜µì…˜
            output_scores=True, # score ê°’ì„ ë°›ê¸° ìœ„í•œ ì˜µì…˜
            num_beams=1,
            do_sample=False
        )
    
        # decoding
        print(tokenizer.batch_decode(output.sequences, skip_special_tokens=True))
        # ["Hello, I'm sorry,", "I love the way you're doing", 'I want to be able to do that']
        ```

#### ì¶œë ¥ê°’

> [!info] Reference
>  https://huggingface.co/docs/transformers/internal/generation_utils#transformers.generation.GreedySearchDecoderOnlyOutput

##### `sequences`

- shape : `torch.LongTensor[batch_size, sequence_length`

- ìƒì„± ê²°ê³¼ ë§Œë“¤ì–´ì§„ sequence
    - ì…ë ¥ìœ¼ë¡œ ì¤€ tokenë“¤ê¹Œì§€ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆë‹¤ëŠ” ì ì— ìœ ì˜
        - ìƒˆë¡œ ìƒì„±ëœ tokenë“¤ë§Œ ë³´ê³  ì‹¶ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì…ë ¥ tokenë“¤ì„ ì˜ë¼ë‚´ê³  ë³´ë©´ ëœë‹¤.

            ```python title:"ìƒˆë¡œ ìƒì„±ëœ tokenë“¤ë§Œ ë³´ê¸°" linenos highlight:"3"
            input_ids_len = tokenized.input_ids.shape[1]

            generated_token_ids = output.sequences[:, input_ids_len:]

            print(tokenizer.batch_decode(generated_token_ids, skip_special_tokens=True))
            # [", I'm sorry,", " the way you're doing", ' be able to do that']
            ```

    - ë‘ ë²ˆì§¸ dimension(`sequence_length`)ì˜ í¬ê¸°ëŠ” ìµœëŒ€ `max_length`ê¹Œì§€ ê°ˆ ìˆ˜ ìˆë‹¤.
        - ë§Œì•½ batch ë‚´ ëª¨ë“  exampleë“¤ì´ `<eos>` tokenì„ ìƒì„±í•´ ìƒì„±ì´ ì¼ì° ì¢…ë£Œë˜ì—ˆë‹¤ë©´ ê·¸ë³´ë‹¤ ì§§ì•„ì§ˆ ìˆ˜ ìˆë‹¤.

##### `scores`

- shape : `tuple(torch.FloatTensor[batch_size, vocab_size])`
- `.generate()` ë©”ì†Œë“œ í˜¸ì¶œ ì‹œ `output_scores=True` ì˜µì…˜ì„ ì£¼ì–´ì•¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- ê° ì‹œì ì—ì„œì˜ logit(LM headì˜ ì¶œë ¥)ì„ ëª¨ì€ tuple
    - tupleì˜ í¬ê¸°ëŠ” ìµœëŒ€ `max_new_token`ê¹Œì§€ ê°ˆ ìˆ˜ ìˆë‹¤.
        - ë§Œì•½ batch ë‚´ ëª¨ë“  exampleë“¤ì´ `<eos>` tokenì„ ìƒì„±í•´ ìƒì„±ì´ ì¼ì° ì¢…ë£Œë˜ì—ˆë‹¤ë©´ ê·¸ë³´ë‹¤ ì§§ì•„ì§ˆ ìˆ˜ ìˆë‹¤.
    - ìœ„ skeleton ì½”ë“œì—ì„œ ë§¤ ì‹œì ë§ˆë‹¤ `next_token_logits`ì„ ëª¨ì€ ê°’

> [!tip] Advanced
> ë§Œì•½ logits processorë¥¼ ì‚¬ìš©í•œë‹¤ë©´, logits processorê°€ ì ìš©ëœ ê°’ì´ ë‹´ê¸´ë‹¤.

##### `attentions`

- shape : `tuple(tuple(torch.FloatTensor[batch_size, num_heads, generated_lengths, sequence_length]))`
- `.generate()` ë©”ì†Œë“œ í˜¸ì¶œ ì‹œ `output_attentions=True` ì˜µì…˜ì„ ì£¼ì–´ì•¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- ê° ì‹œì ì—ì„œì˜, ê° layerë§ˆë‹¤ì˜, attention ê°’ë“¤ì„ ëª¨ì€ tupleë“¤ì˜ tuple

##### `hidden_states`

- shape : `tuple(tuple(torch.FloatTensor[batch_size, generated_length, hidden_size]))`
- `.generate()` ë©”ì†Œë“œ í˜¸ì¶œ ì‹œ `output_hidden_states=True` ì˜µì…˜ì„ ì£¼ì–´ì•¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- ê° ì‹œì ì—ì„œì˜, ê° layerë§ˆë‹¤ì˜, hidden stateë¥¼ ëª¨ì€ tupleë“¤ì˜ tuple

#### ì—¬ë‹´ : transition score

- `model.compute_transition_scores()` ë©”ì†Œë“œë¥¼ ì´ìš©í•˜ë©´ ë§¤ ì‹œì  modelì´ ì–¼ë§ˆë§Œí¼ì˜ í™•ë¥ ë¡œ ë‹¤ìŒ tokenì„ ì¶”ì •í–ˆëŠ”ì§€ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.
    - ì´ ê°’ì„ transition scoreë¼ í•œë‹¤.
    - transition scoreê°€ 0ì´ë¼ë©´ í•´ë‹¹ ì‹œì ì—ì„  ìƒì„±ì´ ì§„í–‰ë˜ì§€ ì•Šì•˜ìŒì„(ì¦‰, `<eos>` tokenì˜ ë“±ì¥ ë“±ìœ¼ë¡œ ì¸í•œ early stopì´ ìˆì—ˆìŒì„) ë‚˜íƒ€ë‚¸ë‹¤.

    ```python title:"compute_transition_scores() ë©”ì†Œë“œ" linenos highlight:"31-36"
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # tokenizer, model ë¶ˆëŸ¬ì˜¤ê¸°
    tokenizer =  AutoTokenizer.from_pretrained("gpt2", padding_side="left")
    tokenizer.pad_token_id = tokenizer.eos_token_id # gpt2ì˜ tokenizerì—ëŠ” pad_tokenì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—, eos_tokenì„ pad_tokenìœ¼ë¡œ ì‚¬ìš©í•˜ë¼ê³  ì•Œë ¤ì¤˜ì•¼ í•œë‹¤.
    
    model = AutoModelForCausalLM.from_pretrained("gpt2", pad_token_id=tokenizer.eos_token_id).to(device) # ë§ˆì°¬ê°€ì§€
    
    # tokenization
    texts = [
        "Hello",
        "I love",
        "I want to"
    ]
    
    tokenized = tokenizer(texts, padding=True, return_tensors="pt").to(device)
    
    # generation
    output = model.generate(
        **tokenized,
        max_new_tokens=5, # (ìµœëŒ€) 5ê°œì˜ tokenë§Œ ìƒì„±
        return_dict_in_generate=True, # ì¶œë ¥ê°’ì„ tuple í˜•íƒœê°€ ì•„ë‹Œ dict í˜•íƒœë¡œ ë°›ê¸° ìœ„í•œ ì˜µì…˜
        output_scores=True, # score ê°’ì„ ë°›ê¸° ìœ„í•œ ì˜µì…˜
        num_beams=1,
        do_sample=False
    )
    
    # transition score ê³„ì‚°
    transition_scores = model.compute_transition_scores(
        sequences=output.sequences,
        scores=output.scores,
        normalize_logits=True
    ) # tensor[batch_size, sequence_length]
    
    # ì²« ë²ˆì§¸ exampleì˜ transition score í™•ì¸
    input_ids_len = tokenized.input_ids.shape[1]
    generated_token_ids = output.sequences[:, input_ids_len:]
    for token, score in zip(generated_token_ids[0], transition_scores[0]):
        print(f"{token:5d} | {tokenizer.decode(token):8s} | {score.item():.3f} | {score.exp().item():.3%}")
        #    11 | ,        | -2.343 | 9.602%
        #   314 |  I       | -2.298 | 10.045%
        #  1101 | 'm       | -1.652 | 19.164%
        #  7926 |  sorry   | -2.471 | 8.453%
        #    11 | ,        | -1.528 | 21.707%
    ```
    
    - ì½”ë“œ ì„¤ëª…
        - line 35 : `normalize_logits=True`ë¥¼ ì£¼ë©´ ê° tokenì˜ logit ê°’ì— log softmaxë¥¼ ì·¨í•œ ê°’ì„ ë°˜í™˜í•œë‹¤. `normalize_logits=False`ë¥¼ ì£¼ë©´ logit ê°’ì´ ê·¸ëŒ€ë¡œ ì¶œë ¥ëœë‹¤.
        - line 42 : `normalize_logits=True`ë¥¼ ì£¼ì—ˆê¸° ë•Œë¬¸ì—, ìœ„ ì˜ˆì œì—ì„œ `transition_scores`ì— ë‹´ê¸´ ê°’ì€ log softmaxê°€ ì·¨í•´ì§„ logit ê°’ì´ë‹¤. ë”°ë¼ì„œ ì´ë¥¼ í™•ë¥  ê°’ìœ¼ë¡œ ë³€í™˜í•˜ë ¤ë©´ `.exp()` ì—°ì‚°ì„ ìˆ˜í–‰í•´ì•¼ í•œë‹¤.

### Beam Search

#### Idea

![beam search](https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png)

- íƒìƒ‰ ì‹œ `num_beams`ê°œì˜ sequence í›„ë³´ë¥¼ ë‘ê³ , ê·¸ ì¤‘ ìµœê³ ë¥¼ ì°¾ëŠ” ë°©ë²•
    - ê° sequence í›„ë³´ë¥¼ "beam"ì´ë¼ í•œë‹¤.
    - ë§¤ ì‹œì , `num_beams`ê°œì˜ beamì— ëŒ€í•´, vocabularyì˜ ê° tokenë“¤ì„ ì´ì–´ë¶™ì¸ `vocab_size`ê°œì˜ sequenceì˜ í™•ë¥ ì„ êµ¬í•œ ë’¤(ì¦‰, `num_beams * vocab_size`ê°œì˜ sequence í™•ë¥ ì„ êµ¬í•œ ë’¤), ì´ ì¤‘ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ sequence `num_beams`ê°œë¥¼ ì¶”ë¦¬ëŠ” ê³¼ì •ì„ ë°˜ë³µ
    - ì˜ˆë¥¼ ë“¤ì–´ `num_beams=2`ì¸ ìœ„ ê·¸ë¦¼ì—ì„  ("The", "dog", "has")ê°€ ì„ íƒëœë‹¤.
        - ì ì„ ì€ greedy searchê°€ íƒìƒ‰í•œ pathë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

- greedy searchì˜ í•œê³„ë¥¼ ê·¹ë³µ
    - ë¯¸ë˜ì— ë” ë‚˜ì€ sequenceë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ `num_beams`ê°œì˜ í›„ë³´ë¥¼ ìœ ì§€
    - top1ì´ ì•„ë‹ˆë¼ topk(k=`num_beams`) ì—°ì‚°ì„ ìˆ˜í–‰
        - ì‹¤ì œ êµ¬í˜„ ì‹œì—ëŠ” k=`2 * num_beams`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ë¶„ì˜ beamì„ ë§Œë“¤ì–´ ë†“ìŒ
    - greedy searchëŠ” `num_beams=1`ì¸ beam searchë¼ ì´í•´í•  ìˆ˜ ìˆìŒ

- batch ì²˜ë¦¬
    - `num_beams`ê°œì˜ ê²½ìš°ì˜ ìˆ˜ë¥¼ ê³ ë ¤í•œë‹¤ëŠ” ê²ƒì€, ê²°êµ­ `num_beams`ë²ˆì˜ inferenceë¥¼ í•´ì•¼ í•œë‹¤ëŠ” ê²ƒ
    - ë”°ë¼ì„œ `batch_size` í¬ê¸°ì˜ batch ì…ë ¥ì— ëŒ€í•´ `num_beams`ë§Œí¼ì˜ beam searchë¥¼ ìˆ˜í–‰í•˜ëŠ” ë¬¸ì œëŠ”, `batch_size * num_beams`í¬ê¸°ì˜ batch ì…ë ¥ì— ëŒ€í•´ inferenceë¥¼ ìˆ˜í–‰í•˜ëŠ” ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆë‹¤. â†’ ë³‘ë ¬í™” ê°€ëŠ¥
    - beam searchë¥¼ ì‹œì‘í•  ë•Œ, ê° exampleì„ `num_beams`ê°œ ë³µì œí•˜ì—¬ `batch_size * num_beams` í¬ê¸°ì˜ batchë¥¼ ë§Œë“¤ê³  inference ì§„í–‰
        - ë‹¤ë§Œ ì´ë ‡ê²Œ í•˜ë©´ ê° beamì—ì„œ ë‚˜ì˜¤ëŠ” ê²°ê³¼ê°’ì´ í•­ìƒ ê°™ì•„ì§€ë¯€ë¡œ, ëª¨ë“  beamì´ ê°™ì€ ê°’ìœ¼ë¡œ ì±„ì›Œì§„ë‹¤ëŠ” ë¬¸ì œê°€ ìˆìŒ
        - ì²« ë²ˆì§¸ beamì˜ ì´ˆê¸° sequence í™•ë¥ ì„ 0ìœ¼ë¡œ, ë‚˜ë¨¸ì§€ beamë“¤ì˜ ì´ˆê¸° sequence scoreë¥¼ `-inf`ë¡œ ì±„ì›Œë†“ìœ¼ë©´ ì´ ë¬¸ì œë¥¼ í”¼í•  ìˆ˜ ìˆìŒ
        - ê·¸ ê²°ê³¼ `beam_indices`ì˜ ì²« ë²ˆì§¸ column(ì¦‰, ìƒì„±ì„ ì‹œì‘í•˜ëŠ” ìµœì´ˆ ì‹œì ì—ì„œ ì„ íƒëœ beamì˜ index)ëŠ” í•­ìƒ 0ì´ ëœë‹¤.

#### ì¥/ë‹¨ì 

- ì¥ì  : 
    - (greedy search ëŒ€ë¹„) ìƒì„±ë˜ëŠ” sequenceì˜ í’ˆì§ˆì´ ì¢‹ë‹¤.
    - ì–¸ì œë‚˜ ê°™ì€ ê°’ì„ ë‚´ë†“ëŠ”ë‹¤(deterministic).
- ë‹¨ì  :
    - ìµœì„ ì˜ ì •ë‹µì´ ì•„ë‹ ìˆ˜ ìˆë‹¤.
        - `num_beams`ê°€ ì‘ìœ¼ë©´ ë”ë”ìš±.
        - ë¬¼ë¡  greedy searchë³´ë‹¤ëŠ” 'ìµœì„ ì— ê°€ê¹Œìš´' ì •ë‹µì„ ì°¾ê¸´ í•œë‹¤.
    - ë©”ëª¨ë¦¬ ì†Œë¹„
        - (greedy search ëŒ€ë¹„) ë©”ëª¨ë¦¬ ì†Œë¹„ê°€ (ìµœì†Œ) `num_beams`ë°° ì»¤ì§„ë‹¤.

#### ì‚¬ìš©ë²•

- ğŸ¤—huggingface transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `generate()` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©´ ì†ì‰½ê²Œ beam searchë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.
    - `.generate()` ë©”ì†Œë“œ í˜¸ì¶œ ì‹œ `num_beams > 1`, `do_sample=False`ë¡œ ì£¼ë©´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
    - `num_return_sequences` ì¸ìë¥¼ ì´ìš©í•´ (ê° example ë‹¹) ë°˜í™˜ë˜ëŠ” sequenceë“¤ì˜ ìˆ˜ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. ì´ ê°’ì€ (ë‹¹ì—°íˆ) `num_beams`ë³´ë‹¤ í´ ìˆ˜ ì—†ë‹¤.

        ```python title:".generate() ë©”ì†Œë“œ ì´ìš©í•´ beam search ìˆ˜í–‰í•˜ê¸°" linenos highlight:"27,28"
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # tokenizer, model ë¶ˆëŸ¬ì˜¤ê¸°
        tokenizer =  AutoTokenizer.from_pretrained("gpt2", padding_side="left")
        tokenizer.pad_token_id = tokenizer.eos_token_id # gpt2ì˜ tokenizerì—ëŠ” pad_tokenì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—, eos_tokenì„ pad_tokenìœ¼ë¡œ ì‚¬ìš©í•˜ë¼ê³  ì•Œë ¤ì¤˜ì•¼ í•œë‹¤.
        
        model = AutoModelForCausalLM.from_pretrained("gpt2", pad_token_id=tokenizer.eos_token_id).to(device) # ë§ˆì°¬ê°€ì§€
        
        # tokenization
        texts = [
            "Hello",
            "I love",
            "I want to"
        ]
        
        tokenized = tokenizer(texts, padding=True, return_tensors="pt").to(device)
        
        # generation
        output = model.generate(
            **tokenized,
            max_new_tokens=5, # (ìµœëŒ€) 5ê°œì˜ tokenë§Œ ìƒì„±
            return_dict_in_generate=True, # ì¶œë ¥ê°’ì„ tuple í˜•íƒœê°€ ì•„ë‹Œ dict í˜•íƒœë¡œ ë°›ê¸° ìœ„í•œ ì˜µì…˜
            output_scores=True, # score ê°’ì„ ë°›ê¸° ìœ„í•œ ì˜µì…˜
            num_beams=5,
            do_sample=False,
            num_return_sequences=4
        )
    
        # decoding
        print(tokenizer.batch_decode(output.sequences, skip_special_tokens=True))
        # ["Hello.\n\nI'm", 'I love it. I love it', 'I want to thank you for your support']
        ```

#### ì¶œë ¥ê°’

> [!info] Reference
>  https://huggingface.co/docs/transformers/internal/generation_utils#transformers.generation.BeamSearchDecoderOnlyOutput

##### `sequences`

- shape : `torch.LongTensor[batch_size * num_return_sequences, sequence_length`

- ìƒì„± ê²°ê³¼ ë§Œë“¤ì–´ì§„ sequence
    - ì…ë ¥ìœ¼ë¡œ ì¤€ tokenë“¤ê¹Œì§€ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆë‹¤ëŠ” ì ì— ìœ ì˜
    - ì²« ë²ˆì§¸ dimension(`batch_size * num_return_sequences`)ì´ ë‹¨ìˆœíˆ `batch_size`ê°€ ì•„ë‹ˆë¼ëŠ” ì ì— ìœ ì˜
        - ë‹¤ë§Œ ìˆœì„œëŠ” ë³´ì¥ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´ `batch_size=3`, `num_return_sequence=4`ë¼ë©´, `output.sequences[0:4]`ëŠ” ì²« ë²ˆì§¸ example, `output.sequences[4:8]`ì€ ë‘ ë²ˆì§¸ example, `output.sequences[8:12]`ëŠ” ì„¸ ë²ˆì§¸ exampleì— ëŒ€í•œ ìƒì„±ì´ë‹¤.
    - ë‘ ë²ˆì§¸ dimension(`sequence_length`)ì˜ í¬ê¸°ëŠ” ìµœëŒ€ `max_length`ê¹Œì§€ ê°ˆ ìˆ˜ ìˆë‹¤.
        - ë§Œì•½ batch ë‚´ ëª¨ë“  exampleë“¤ì´ `<eos>` tokenì„ ìƒì„±í•´ ìƒì„±ì´ ì¼ì° ì¢…ë£Œë˜ì—ˆë‹¤ë©´ ê·¸ë³´ë‹¤ ì§§ì•„ì§ˆ ìˆ˜ ìˆë‹¤.

##### `sequences_scores`

- shape : `torch.FloatTensor[batch_size * num_return_sequences]`
- `.generate()` ë©”ì†Œë“œ í˜¸ì¶œ ì‹œ `output_scores=True` ì˜µì…˜ì„ ì£¼ì–´ì•¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- ìƒì„±ëœ sequenceì˜ ìµœì¢… score
    - ê° tokenë“¤ ì‚¬ì´ì˜ transition scoreì˜ í•©ì— length penaltyë¥¼ ì ìš©í•œ ê°’
        
        $$\textrm{score} = \frac{1}{t}\sum_i
{ #t}
 \log p_{\theta} (x_i | x_{<i})$$
        
        - $\log p_{\theta} (x_i | x_{<i})$ : transition score, $t$ : sequenceì˜ ê¸¸ì´
        - transition scoreëŠ” logit ê°’ì— log softmaxë¥¼ ì·¨í•œ ê°’ì´ë¯€ë¡œ, ì´ë“¤ì„ ë”í•˜ë©´ sequenceì˜ í™•ë¥ ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤(ì •í™•íˆëŠ”, sequence í™•ë¥ ì— logë¥¼ ì”Œìš´ ê°’(= sequence score)ì„ ì–»ì„ ìˆ˜ ìˆë‹¤).
        - ë‹¤ë§Œ ë‹¨ìˆœíˆ ë”í•˜ê¸°ë§Œ í•˜ë©´ ê¸¸ì´ê°€ ê¸´ sequenceì¼ìˆ˜ë¡ í™•ë¥ ì´ ë‚®ê²Œ ë‚˜ì˜¤ë¯€ë¡œ, transition scoreë“¤ì˜ í•©ì„ sequenceì˜ ê¸¸ì´ë¡œ ë‚˜ëˆ  ì¤€ë‹¤(length penalty)
            - sequenceì˜ ê¸¸ì´ëŠ” ì…ë ¥ê¹Œì§€ í¬í•¨í•œ ê¸¸ì´ë¼ëŠ” ì ì— ìœ ì˜

        > [!tip] Advanced
        > ë§Œì•½ `length_penalty` $l$ì„ ì„¤ì •í•œ ê²½ìš° sequence scoreëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤.
        > 
        > $$\textrm{score} = \frac{1}{t \cdot l}\sum_i
{ #t}
 \log p_{\theta} (x_i | x_{<i})$$
        
        ```python title:"sequence score ì§ì ‘ ê³„ì‚°í•˜ê¸°" linenos highlight:"44"
        import torch
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # tokenizer, model ë¶ˆëŸ¬ì˜¤ê¸°
        tokenizer =  AutoTokenizer.from_pretrained("gpt2", padding_side="left")
        tokenizer.pad_token_id = tokenizer.eos_token_id # gpt2ì˜ tokenizerì—ëŠ” pad_tokenì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì—, eos_tokenì„ pad_tokenìœ¼ë¡œ ì‚¬ìš©í•˜ë¼ê³  ì•Œë ¤ì¤˜ì•¼ í•œë‹¤.
        
        model = AutoModelForCausalLM.from_pretrained("gpt2", pad_token_id=tokenizer.eos_token_id).to(device) # ë§ˆì°¬ê°€ì§€
        
        # tokenization
        texts = [
            "Hello",
            "I love",
            "I want to"
        ]
        
        tokenized = tokenizer(texts, padding=True, return_tensors="pt").to(device)
        
        # generation
        output = model.generate(
            **tokenized,
            max_new_tokens=5, # (ìµœëŒ€) 5ê°œì˜ tokenë§Œ ìƒì„±
            return_dict_in_generate=True, # ì¶œë ¥ê°’ì„ tuple í˜•íƒœê°€ ì•„ë‹Œ dict í˜•íƒœë¡œ ë°›ê¸° ìœ„í•œ ì˜µì…˜
            output_scores=True, # score ê°’ì„ ë°›ê¸° ìœ„í•œ ì˜µì…˜
            num_beams=5,
            do_sample=False,
            num_return_sequences=4
        )
        
        # transition score ê³„ì‚°
        transition_scores = model.compute_transition_scores(
            sequences=output.sequences,
            scores=output.scores,
            beam_indices=output.beam_indices,
            normalize_logits=True
        ) # tensor[batch_size * num_return_sequences, sequence_length]

        # sequence score ê³„ì‚°
        input_ids_len = tokenized.input_ids.shape[1]
        output_len = input_ids_len + (transition_scores < 0).sum(axis=1)
        
        calculated_sequences_scores = transition_scores.sum(axis=1) / output_len
        
        for i, (calculated_sequence_score, sequence_score) in enumerate(zip(calculated_sequences_scores, output.sequences_scores)):
            print(f"example #{i:02d} | {calculated_sequence_score:.4f} | {sequence_score:.4f} | {torch.isclose(calculated_sequence_score, sequence_score)}")
            # example #00 | -1.1756 | -1.1756 | True
            # example #01 | -1.1891 | -1.1891 | True
            # example #02 | -1.2796 | -1.2796 | True
            # example #03 | -1.2798 | -1.2798 | True
            # example #04 | -1.0159 | -1.0159 | True
            # example #05 | -1.0794 | -1.0794 | True
            # example #06 | -1.0991 | -1.0991 | True
            # example #07 | -1.1187 | -1.1187 | True
            # example #08 | -1.0093 | -1.0093 | True
            # example #09 | -1.0591 | -1.0591 | True
            # example #10 | -1.0797 | -1.0797 | True
            # example #11 | -1.0971 | -1.0971 | True
        ```

        - ì½”ë“œ ì„¤ëª…
            - line 36 : (greedy search ë•Œì™€ëŠ” ë‹¤ë¥´ê²Œ) beam searchì—ì„œëŠ” `beam_indices` í•­ëª©ë„ ë„˜ê²¨ì¤˜ì•¼ transition scoreë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.
            - line 42 : `<eos>` token ì´í›„ì˜ tokenë“¤ì˜ transition scoreëŠ” 0ì´ë¯€ë¡œ, 0ë³´ë‹¤ ì‘ì€ í•­ëª©ë“¤ì˜ ê°œìˆ˜ë¥¼ ì„¸ë©´ ìƒì„±ëœ sequenceì˜ ì‹¤ì œ ê¸¸ì´ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.
            - line 47 : ë¶€ë™ì†Œìˆ˜ì  ë¬¸ì œë¡œ ì¸í•´ float ê°„ì˜ ë¹„êµëŠ” ë‹¨ìˆœíˆ `==`ë¡œ í•  ìˆ˜ ì—†ë‹¤. `torch.isclose()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ ë‘ ê°’ì´ ì¶©ë¶„íˆ ê°€ê¹Œìš´ì§€ ë¹„êµí–ˆë‹¤.

##### `scores`

- shape : `tuple(torch.FloatTensor[batch_size * num_beams * num_return_sequences, vocab_size])`
- `.generate()` ë©”ì†Œë“œ í˜¸ì¶œ ì‹œ `output_scores=True` ì˜µì…˜ì„ ì£¼ì–´ì•¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- ê° ì‹œì ì—ì„œì˜ logit(LM headì˜ ì¶œë ¥)ì„ ëª¨ì€ tuple
    - tupleì˜ í¬ê¸°ëŠ” ìµœëŒ€ `max_new_token`ê¹Œì§€ ê°ˆ ìˆ˜ ìˆë‹¤.
        - ë§Œì•½ batch ë‚´ ëª¨ë“  exampleë“¤ì´ `<eos>` tokenì„ ìƒì„±í•´ ìƒì„±ì´ ì¼ì° ì¢…ë£Œë˜ì—ˆë‹¤ë©´ ê·¸ë³´ë‹¤ ì§§ì•„ì§ˆ ìˆ˜ ìˆë‹¤.

##### `beam_indices`

- shape : `torch.LongTensor[batch_size * num_return_sequences, sequence_length]`
- `.generate()` ë©”ì†Œë“œ í˜¸ì¶œ ì‹œ `output_scores=True` ì˜µì…˜ì„ ì£¼ì–´ì•¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- ê° ì‹œì ì—ì„œ ì„ íƒëœ `num_beams`ê°œì˜ sequenceê°€, ì´ì „ ì‹œì ì—ì„œ ë§Œë“¤ì–´ì§„ beamë“¤ ì¤‘ ëª‡ ë²ˆì§¸ beamì— ìƒˆ tokenì„ ë¶™ì—¬ ë§Œë“¤ì–´ ì§„ ê²ƒì¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” tensor
    - ì˜ˆë¥¼ ë“¤ì–´ `beam_indices[i, j] = k`ë¼ëŠ” ë§ì€, ì‹œì  `j`ì— ë§Œë“¤ì–´ì§„ `i`ë²ˆì§¸ sequenceëŠ”, ì´ì „ ì‹œì ì˜ `k`ë²ˆì§¸ sequenceì— ìƒˆë¡œìš´ tokenì„ ë¶™ì—¬ ë§Œë“¤ì–´ì§„ ê²ƒì„ì„ ë‚˜íƒ€ëƒ„

##### `attentions`

- shape : `tuple(tuple(torch.FloatTensor[batch_size * num_beams, num_heads, generated_lengths, sequence_length]))`
- `.generate()` ë©”ì†Œë“œ í˜¸ì¶œ ì‹œ `output_attentions=True` ì˜µì…˜ì„ ì£¼ì–´ì•¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- ê° ì‹œì ì—ì„œì˜, ê° layerë§ˆë‹¤ì˜, attention ê°’ë“¤ì„ ëª¨ì€ tupleë“¤ì˜ tuple

##### `hidden_states`

- shape : `tuple(tuple(torch.FloatTensor[batch_size * num_beams * num_return_sequences, generated_length, hidden_size]))`
- `.generate()` ë©”ì†Œë“œ í˜¸ì¶œ ì‹œ `output_hidden_states=True` ì˜µì…˜ì„ ì£¼ì–´ì•¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- ê° ì‹œì ì—ì„œì˜, ê° layerë§ˆë‹¤ì˜, hidden stateë¥¼ ëª¨ì€ tupleë“¤ì˜ tuple